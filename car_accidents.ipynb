{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Traffic Accident Dataset Exploration\n",
    "Grant Hanley\n",
    "\n",
    "Working Analysis \n",
    "\n",
    "Last Updated: JAN 2 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Motivation\n",
    "\n",
    "Car accidents are a major public health concern, causing significant numbers of deaths and injuries every year. According to the Centers for Disease Control and Prevention (CDC), motor vehicle accidents are the leading cause of death for people aged 1-54 in the United States. In 2019, over 36,000 people died in car accidents in the US, and millions more were injured. These accidents not only have devastating personal and emotional impacts, but they also have significant economic costs. The National Highway Traffic Safety Administration estimates that the economic cost of motor vehicle crashes in the US was over $800 billion in 2020, including costs such as medical expenses, lost productivity, and property damage. Reducing the number of car accidents, deaths, and injuries is therefore a crucial goal, with the potential to save lives and reduce economic burden.\n",
    "\n",
    "## Application\n",
    "\n",
    "Geo-spatial analytics and data science techniques can play a vital role in identifying where reoccurring car accidents are happening. Doing so may help to identify local interventions which may support reducing the number of accidents. By analyzing accident data with geographic information, it is possible to identify patterns and trends in the location and frequency of accidents. For example, geo-spatial data analysis can be used to identify hotspots, or areas with a high concentration of accidents, which may be caused by factors such as poor road design, high traffic volumes, or high speeds.\n",
    "\n",
    "Data science techniques, such as machine learning and predictive modeling, can also be used to identify factors that contribute to the likelihood of an accident occurring at a particular location. For example, data on weather conditions, road conditions, traffic volumes, and other factors can be used to build models that predict the likelihood of an accident occurring at a particular location. This information can be used to inform the development of interventions aimed at reducing the number of accidents in high-risk areas.\n",
    "\n",
    "By identifying where reoccurring accidents are occurring and understanding the factors that contribute to these accidents, geo-spatial analytics and data science techniques can help policymakers and other stakeholders develop targeted interventions to reduce the number of car accidents, deaths, and injuries. These interventions may include measures such as improved road design, traffic calming measures, or educational campaigns to promote safe driving practices.\n",
    "\n",
    "## The Data \n",
    "This dataset was found on Kaggle here: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents?resource=download. The authors request the following citations: \n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019. \n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n",
    "\n",
    "In their words, \"This is a countrywide car accident dataset, which covers 49 states of the USA. The accident data are collected from February 2016 to Dec 2021, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 2.8 million accident records in this dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from mapclassify import classify\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import statistics\n",
    "from scipy import ndimage\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "import folium.plugins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Large Dataset ~1.1 GB using chunksize\n",
    "#Dataset found here: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/download?datasetVersionNumber=12\n",
    "reader = pd.read_csv('US_Accidents_Dec21_updated.csv', iterator=True, chunksize=10000)\n",
    "\n",
    "#use pd.concat to get all 2.8mil rows, takes ~2 mins...apparently dask can improve efficiency\n",
    "df = pd.concat(reader, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Car Accident dataset is over 1GB and accordingly requires the use of an iterator and being broken into smaller chunks to be read into a pandas dataframe. After reading in the TextFileReader object, the reader can be concatenated into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the maximum number of columns, show the first few data points\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check lengths and data types\n",
    "print(len(df))\n",
    "print(df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current car accident dataset consists of over 2.8 million car accidents. The information contained includes geocoded spatial-temporal data with a number of other variables representing weather attributes at the time of the accident and characterization of the traffic road features in the area where the accidents occured."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to state of virginia only\n",
    "df = df[(df['State'] == 'VA')]\n",
    "\n",
    "#narrow to a county, use geopandas to load a county polygon, filter to accidents within the county polygon\n",
    "#will require a later merge with county shapefile\n",
    "#df = df[(df['State'] == 'VA') & df['County'] == 'Prince William']\n",
    "\n",
    "#consider zipcode filtering\n",
    "#merge with ZCTA from Census.gov\n",
    "\n",
    "#extract I-95 lines from open street map, create a buffer polygon around the line, filter to accidents within the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round off the 'Start_Lat' and 'Start_Lng' variables to allow for area buffer that an accident began\n",
    "df['lat'] = df['Start_Lat'].round(4)\n",
    "df['lon'] = df['Start_Lng'].round(4)\n",
    "\n",
    "# Change data type of severity to an integer to support filtering \n",
    "df[\"Severity\"] = df[\"Severity\"].astype(int)\n",
    "\n",
    "#groupby but show the top 10 locations instead of the top 10 data points\n",
    "grouped_count = df.groupby(['lon', 'lat']).size().rename('count').reset_index()\n",
    "grouped_count.sort_values(by='count', ascending=False).head(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable 'lat_lon' with combined lat longs to support merging\n",
    "grouped_count['lat_lon']= list(zip(grouped_count['lat'], grouped_count['lon']))\n",
    "df['lat_lon']= list(zip(df['lat'], df['lon']))\n",
    "\n",
    "# Merge the original df and grouped_count df to maintain count information as a feature for all data points\n",
    "# Use a left join to retain all information in the Car Accident Dataset\n",
    "df = pd.merge(df, grouped_count, on ='lat_lon', how ='left')\n",
    "\n",
    "# Drop added variables created from the join, rename variables changed from the merge\n",
    "df = df.drop(['lon_y','lat_y'], axis=1).rename(columns={'lat_x':'lat', 'lon_x':'lon'})\n",
    "\n",
    "# Display sample .head()\n",
    "df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Time Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to pandas date time format\n",
    "df['Start_Time'] = df['Start_Time'].apply(pd.to_datetime)\n",
    "df['End_Time'] = df['End_Time'].apply(pd.to_datetime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a geometry column \n",
    "point_geometry = [Point(xy) for xy in zip(df['Start_Lng'], df['Start_Lat'])]\n",
    "\n",
    "# Coordinate reference system : WGS84\n",
    "crsys = {'init': 'epsg:4326'}\n",
    "\n",
    "# Creating a Geographic data frame with date time included, spatial temporal\n",
    "df = gpd.GeoDataFrame(df, crs=crsys, geometry=point_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the average accident rate across all space, find the ~top 5% of accident locations\n",
    "#consider removing, consider the raster approach in lieu of count approach, better for later ML applications\n",
    "avg_accident_rate = grouped_count.sort_values(by='count', ascending=False)['count'].mean()\n",
    "std_accident_rate = grouped_count.sort_values(by='count', ascending=False)['count'].mean()\n",
    "twosigma = avg_accident_rate+2*std_accident_rate\n",
    "\n",
    "#groupby lat and long, create count variable\n",
    "hotspots = df.groupby(['lon', 'lat']).size().rename('count').reset_index().sort_values(by='count', ascending=False).head(50)\n",
    "hotspots = hotspots[(hotspots['count']>= twosigma)]\n",
    "\n",
    "#plotly figure with hotspots identified\n",
    "#create point map or scatter mapbox from plotly and customize layout options\n",
    "hotspotmap = px.scatter_mapbox(\n",
    "    hotspots,\n",
    "    lat=\"lat\",\n",
    "    lon=\"lon\",\n",
    "    color = \"count\",\n",
    "    size = \"count\",\n",
    "    hover_name= \"count\",\n",
    "    hover_data=[\"lon\", \"lat\", \"count\"],\n",
    "    zoom=12,\n",
    "    height=400,\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism\n",
    ")\n",
    "hotspotmap.update_layout(mapbox_style=\"open-street-map\")\n",
    "hotspotmap.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "\n",
    "#display the point map\n",
    "hotspotmap.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifc Location Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change data type of severity to an integer to support filtering \n",
    "df[\"Severity\"] = df[\"Severity\"].astype(int)\n",
    "\n",
    "# Focus in on a major slow down area in the I95 cooridoor\n",
    "focus = df[(df['Start_Lat'] >= 38.66) & (df['Start_Lat'] <= 38.71) & (df['Start_Lng'] >= -77.26) & (df['Start_Lng'] <= -77.21)]\n",
    "\n",
    "focus['Start_Time'] = focus['Start_Time'].apply(pd.to_datetime)\n",
    "focus['End_Time'] = focus['End_Time'].apply(pd.to_datetime)\n",
    "\n",
    "#create point map or scatter mapbox from plotly and customize layout options\n",
    "pointmap = px.scatter_mapbox(\n",
    "    focus,\n",
    "    lat=\"Start_Lat\",\n",
    "    lon=\"Start_Lng\",\n",
    "    color = \"Severity\",\n",
    "    hover_name= \"Severity\",\n",
    "    hover_data=[\"lon\", \"lat\", \"count\"],\n",
    "    zoom=12,\n",
    "    height=400,\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism\n",
    ")\n",
    "pointmap.update_layout(mapbox_style=\"open-street-map\")\n",
    "pointmap.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "\n",
    "#display the point map\n",
    "pointmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap function from geopandas documentation\n",
    "\n",
    "def heatmap(d, bins=(100,100), smoothing=1.3, cmap='jet'):\n",
    "    def getx(pt):\n",
    "        return pt.coords[0][0]\n",
    "\n",
    "    def gety(pt):\n",
    "        return pt.coords[0][1]\n",
    "\n",
    "    x = list(d.geometry.apply(getx))\n",
    "    y = list(d.geometry.apply(gety))\n",
    "    heatmap, xedges, yedges = np.histogram2d(y, x, bins=bins)\n",
    "    extent = [yedges[0], yedges[-1], xedges[-1], xedges[0]]\n",
    "\n",
    "    logheatmap = np.log(heatmap)\n",
    "    logheatmap[np.isneginf(logheatmap)] = 0\n",
    "    logheatmap = ndimage.filters.gaussian_filter(logheatmap, smoothing, mode='nearest')\n",
    "    \n",
    "    plt.imshow(logheatmap, cmap=cmap, extent=extent)\n",
    "    plt.colorbar()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(focus, bins=50, smoothing=1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Point Map\n",
    "\n",
    "Point maps are also useful for visualizing data because they allow you to see the data in relation to the geography of the area being analyzed. This can be particularly useful for understanding the context of the data and for identifying spatial patterns and trends.\n",
    "\n",
    "Here a point map is used with the data separated categorically by car accident severity as an initial attempt to gain understand where the car accidents are reoccuring. \n",
    "\n",
    "- Severe Virginia car accidents using pandas and plotly\n",
    "- build in time slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#color by severity, required to be a string to be categroical\n",
    "df[\"Severity\"] = df[\"Severity\"].astype(str)\n",
    "\n",
    "#create point map or scatter mapbox from plotly and customize layout options\n",
    "pointmap = px.scatter_mapbox(\n",
    "    df,\n",
    "    lat=\"Start_Lat\",\n",
    "    lon=\"Start_Lng\",\n",
    "    color=\"Severity\",\n",
    "    hover_name=\"ID\",\n",
    "    hover_data=[\"Start_Time\",\"Severity\",\"City\",\"State\", \"Zipcode\", \"Street\"],\n",
    "    zoom=6,\n",
    "    height=400,\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism\n",
    ")\n",
    "pointmap.update_layout(mapbox_style=\"open-street-map\")\n",
    "pointmap.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "\n",
    "#display the point map\n",
    "pointmap.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geo-Spatial  Heatmap\n",
    "\n",
    "A geospatial heatmap is a type of heatmap that can be used to visualize data with a geographic component. A geospatial heatmap typically uses a color scale to represent the values of the data. Geospatial heatmaps are particularly useful for identifying spatial patterns and trends in data, as they allow you to see how the data varies across different locations. The hotspots in the spatial heatmap here represent locations where multiple accidents are reoccuring within close proximity.  The hotspots are useful for identifying areas of the data that may require further investigation or analysis.\n",
    "\n",
    "- Heatmap of Virginia car accidents using pandas, statistics, and plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the center lat and long for the data in our set to center the map using the statistics package\n",
    "meanLong = statistics.mean(df['Start_Lng'])\n",
    "meanLat = statistics.mean(df['Start_Lat'])\n",
    "\n",
    "#create a heatmap of the data\n",
    "heatmap = px.density_mapbox(df, lat='lat', lon='lon', z='count', radius=7,\n",
    "                        center=dict(lat=meanLat, lon=meanLong), zoom=6,\n",
    "                        mapbox_style=\"open-street-map\")\n",
    "heatmap.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "heatmap.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore GeoPandas, Shapely and other Geospatial packages**\n",
    "- Include geospatial polygons on a plotly map, county or zipcode narrow\n",
    "- Do some demons on fundamental use of these packages. \n",
    "\n",
    "- Shapely: https://shapely.readthedocs.io/en/stable/: This package provides tools for working with geometric objects such as points, lines, and polygons. It is often used in conjunction with Geopandas to perform spatial operations on geospatial data.\n",
    "- GeoPandas: https://geopandas.org/en/stable/: This package allows you to manipulate and analyze geospatial data stored in the form of dataframes, similar to the way you would work with regular pandas dataframes. It is built on top of the popular pandas package and allows you to easily perform operations such as merging, intersecting, and dissolving geospatial data.\n",
    "    - examine interactive mapping via .explore() with folium map objects, https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html\n",
    "    - conduct spatial joins\n",
    "\n",
    "- Rasterio: https://rasterio.readthedocs.io/en/stable/: This package allows you to read, write, and manipulate raster data such as satellite imagery or terrain data. It is designed to work with the popular numpy package and can be used to perform operations such as image processing and analysis.\n",
    "\n",
    "- Folium: https://python-visualization.github.io/folium/: This package allows you to create interactive maps using the Leaflet.js library and display them in a Jupyter notebook or web page. It is particularly useful for visualizing geospatial data and can be used to create choropleth maps, heatmaps, and marker maps.\n",
    "     - Folium appears to be better than plotly for intereactive html integration with geopandas and raster overlays\n",
    "     - examine add tile layer\n",
    "     - get mapbox api, for satellite layer\n",
    "     - examine add controls for layers \n",
    "     - demo work with map objects, https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html \n",
    "     - add a time slider to folium map\n",
    "\n",
    "- GDAL/OGR: https://gdal.org/: This package provides a set of tools for reading, writing, and manipulating geospatial data in a variety of formats. It is a powerful library that is widely used in the GIS community and is often used in conjunction with other packages such as Rasterio and Geopandas.\n",
    "\n",
    "- SciPy: https://docs.scipy.org/doc/scipy/reference/index.html: SciPy is a collection of open source scientific and technical computing tools for Python. It is built on top of the NumPy library and includes a variety of submodules for tasks such as optimization, linear algebra, signal processing, and more.\n",
    "\n",
    "**OpenStreetMap Feature Extraction**\n",
    "- use osm data to extract features at the specific location\n",
    "\n",
    "**What OSM features are associated with high density accident locations?**\n",
    "- extract road/street features features\n",
    "    - add buff to street lines, filter by \n",
    "- extract buildings to create a raster \n",
    "    - supports object detection refinement\n",
    "- create a raster from osm extracted features\n",
    "- create a raster from the accident data\n",
    "    - representative of accident likelihood\n",
    "\n",
    "**Additional Timeseries Analysis**\n",
    "\n",
    "**Machine Learning Model**\n",
    "- Build a probability of car accident model from nearby osm features\n",
    "    - focus in on the top 10-20 most significant locations in northern va\n",
    "    - extract nearby features\n",
    "    - rasterize features \n",
    "    - build rasters into a model\n",
    "\n",
    "**Computer Vision Integration**\n",
    "- Imagery acquisition\n",
    "- bring in raster imagery of those locations\n",
    "- plot specific accidents on satellite imagery\n",
    "    - consider a mapbox api account\n",
    "    - consider transitioning to folium for this: https://vexceldata.com/lets-get-technical-using-web-map-tiles-in-python-pt-1/ \n",
    "- use opencv and conduct object detection on imagery the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather zcta geometries from census.gov\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read in a shapefile of US Zip code tabulation areas from url, good for repeating, not dependent on local storage and file path\n",
    "gdf_zips = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\")\n",
    "\n",
    "# Print the first few rows of the data\n",
    "gdf_zips.head()\n",
    "len(gdf_zips)\n",
    "\n",
    "# Need to use ZCTA to zipcode crosswalk to conduct geometry merge: https://udsmapper.org/zip-code-to-zcta-crosswalk/\n",
    "\n",
    "# Merge with another dataframe with reference data...\n",
    "\n",
    "# Display polygons...\n",
    "\n",
    "# Apply to Virginia...Zips\n",
    "\n",
    "# Create a filter from the shape files, area filter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenStreetMap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overpy: A Python wrapper for the Overpass API, which allows you to fetch OSM data for a given area and perform queries on the data.\n",
    "\n",
    "This modified version of the extract_features function accepts an additional input parameter, distance, which allows the user to define the distance for the bounding box. The distance parameter has a default value of 50 km, which is used if no value is provided. If a location description is provided as input, the osmnx package is used to generate a bounding box from the location using the specified distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import overpy\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "\n",
    "#def bounding_box(point, distance=5000):\n",
    "#   x_coordinates, y_coordinates = zip(*point)\n",
    "#   return [(min(x_coordinates), min(y_coordinates)), (max(x_coordinates), max(y_coordinates))]\n",
    "\n",
    "def extract_features(bbox_or_location, distance=5000):\n",
    "    \"\"\"\n",
    "    This code defines a extract_features function that takes a bounding box as input and returns a DataFrame with the extracted features.\n",
    "    The function fetches data for the bounding box using the Overpass API and extracts the nodes, ways, and relations from the result. \n",
    "    The extracted features are stored in a list, which is then converted to a DataFrame and returned by the function.\n",
    "\n",
    "    This will fetch data for the specified bounding box and return a DataFrame with the extracted features. \n",
    "    You can then use the DataFrame to analyze, modify, or visualize the OSM data as needed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If bbox_or_location is a string, use osmnx to generate a bounding box\n",
    "    #if isinstance(bbox_or_location, str):\n",
    "    #    point = ox.geocode(bbox_or_location)\n",
    "    #    bbox = bounding_box(point, distance=distance)\n",
    "    #else:\n",
    "    bbox = bbox_or_location\n",
    "\n",
    "    api = overpy.Overpass()\n",
    "\n",
    "    # Fetch data for the bounding box\n",
    "    result = api.query(\"\"\"\n",
    "        node({0});\n",
    "        way({0});\n",
    "        relation({0});\n",
    "        (._;>;);\n",
    "        out body;\n",
    "    \"\"\").format(bbox)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Extract nodes, ways, and relations from the result\n",
    "    for node in result.nodes:\n",
    "        features.append({\"type\": \"node\", \"id\": node.id, \"lat\": node.lat, \"lon\": node.lon, \"tags\": node.tags})\n",
    "\n",
    "    for way in result.ways:\n",
    "        features.append({\"type\": \"way\", \"id\": way.id, \"nodes\": way.nodes, \"tags\": way.tags})\n",
    "\n",
    "    for relation in result.relations:\n",
    "        features.append({\"type\": \"relation\", \"id\": relation.id, \"members\": relation.members, \"tags\": relation.tags})\n",
    "\n",
    "    # Convert the list of features to a DataFrame\n",
    "    df = pd.DataFrame(features)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 50] Network is down",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/gdhan/Documents/Data Science/Python/Car_Accidents/car_accidents.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m extract_features(bbox_or_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m38.76, -77.19, 38.81, -77.13\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_rows\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test\n",
      "\u001b[1;32m/Users/gdhan/Documents/Data Science/Python/Car_Accidents/car_accidents.ipynb Cell 34\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(bbox_or_location, distance)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m bbox \u001b[39m=\u001b[39m bbox_or_location\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Fetch data for the bounding box\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m result \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m    node(\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m);\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m    way(\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m);\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m    relation(\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m);\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m    (._;>;);\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m    out body;\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(bbox))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gdhan/Documents/Data%20Science/Python/Car_Accidents/car_accidents.ipynb#X62sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Extract nodes, ways, and relations from the result\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/overpy/__init__.py:137\u001b[0m, in \u001b[0;36mOverpass.query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    135\u001b[0m response \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunk_size)\n\u001b[1;32m    136\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_chunk_size)\n\u001b[1;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:497\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[0;32m--> 497\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_readinto_chunked(b)\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    501\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:602\u001b[0m, in \u001b[0;36mHTTPResponse._readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mreturn\u001b[39;00m total_bytes \u001b[39m+\u001b[39m n\n\u001b[1;32m    601\u001b[0m temp_mvb \u001b[39m=\u001b[39m mvb[:chunk_left]\n\u001b[0;32m--> 602\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_readinto(temp_mvb)\n\u001b[1;32m    603\u001b[0m mvb \u001b[39m=\u001b[39m mvb[n:]\n\u001b[1;32m    604\u001b[0m total_bytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:642\u001b[0m, in \u001b[0;36mHTTPResponse._safe_readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    640\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadinto(temp_mvb)\n\u001b[1;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(mvb)\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n:\n\u001b[1;32m    644\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mbytes\u001b[39m(mvb[\u001b[39m0\u001b[39m:total_bytes]), \u001b[39mlen\u001b[39m(b))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 50] Network is down"
     ]
    }
   ],
   "source": [
    "test = extract_features(bbox_or_location=\"38.76, -77.19, 38.81, -77.13\")\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tags</th>\n",
       "      <th>nodes</th>\n",
       "      <th>members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node</td>\n",
       "      <td>272594</td>\n",
       "      <td>39.0160483</td>\n",
       "      <td>-77.0154402</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>node</td>\n",
       "      <td>272596</td>\n",
       "      <td>39.0161025</td>\n",
       "      <td>-77.0191980</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>node</td>\n",
       "      <td>272597</td>\n",
       "      <td>39.0162411</td>\n",
       "      <td>-77.0200952</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>node</td>\n",
       "      <td>272611</td>\n",
       "      <td>39.0169965</td>\n",
       "      <td>-77.0255548</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>node</td>\n",
       "      <td>272612</td>\n",
       "      <td>39.0169204</td>\n",
       "      <td>-77.0269442</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type      id         lat          lon tags nodes members\n",
       "0  node  272594  39.0160483  -77.0154402   {}   NaN     NaN\n",
       "1  node  272596  39.0161025  -77.0191980   {}   NaN     NaN\n",
       "2  node  272597  39.0162411  -77.0200952   {}   NaN     NaN\n",
       "3  node  272611  39.0169965  -77.0255548   {}   NaN     NaN\n",
       "4  node  272612  39.0169204  -77.0269442   {}   NaN     NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Accident Patterns\n",
    "\n",
    "### Weekly Accident Patterns\n",
    "\n",
    "### Seasonal Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working With Rasters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a raster file from point data\n",
    "-interpolate point data\n",
    "Extract raster values at a specific location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define interpolation inputs\n",
    "points = list(zip(df.Start_Lat,df.Start_Lng))\n",
    "df['value'] = np.ones((len(df),1))\n",
    "values = df.value\n",
    "\n",
    "#define raster resolution\n",
    "rRes = .1\n",
    "\n",
    "#create coord ranges over the desired raster extension\n",
    "yRange = np.arange(df.Start_Lat.min(),df.Start_Lat.max()+rRes,rRes)\n",
    "xRange = np.arange(df.Start_Lng.min(),df.Start_Lng.max()+rRes,rRes)\n",
    "\n",
    "#create arrays of x,y over the raster extension\n",
    "gridX,gridY = np.meshgrid(xRange, yRange)\n",
    "\n",
    "#interpolate over the grid\n",
    "grid_z = griddata(points, values, (gridX,gridY), method='cubic')\n",
    "\n",
    "print(grid_z.shape)\n",
    "print((df.Start_Lat.min(),df.Start_Lat.max()))\n",
    "print((df.Start_Lng.min(),df.Start_Lng.max()))\n",
    "\n",
    "plt.imshow(grid_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.raster_layers import ImageOverlay\n",
    "\n",
    "# Load the raster data\n",
    "data = grid_z  # Load the raster data from a file or source\n",
    "\n",
    "# Define the bounds of the raster data\n",
    "bounds = ...  # Define the bounds of the raster data in latitude and longitude coordinates\n",
    "\n",
    "# Create a folium Map object\n",
    "m = folium.Map(location=[meanL, longitude], zoom_start=12)\n",
    "\n",
    "# Add the raster data as an image overlay to the map\n",
    "ImageOverlay(data, bounds, opacity=0.5).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Folium Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap, MiniMap\n",
    "import statistics\n",
    "\n",
    "# Change data type of severity to an integer to support filtering \n",
    "df[\"Severity\"] = df[\"Severity\"].astype(int)\n",
    "\n",
    "\n",
    "# create base map object using Map()\n",
    "mapObj = folium.Map(location=[statistics.mean(df['Start_Lat']),statistics.mean(df['Start_Lng'])], tiles=\"OpenStreetMap\", zoom_start = 7, control_scale = True)\n",
    "\n",
    "# Add point markers\n",
    "\n",
    "# loop through the rows of the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # get the values of the latitude and longitude columns\n",
    "    lat = row[\"Start_Lat\"]\n",
    "    lng = row[\"Start_Lng\"]\n",
    "    severity = row[\"Severity\"]\n",
    "\n",
    "    # create a circle marker at the latitude, longitude location\n",
    "    marker = folium.CircleMarker(location=[lat, lng], radius=2, color=\"red\", fill=True, fill_color=\"red\", fill_opacity=severity / 10)\n",
    "    marker.add_to(mapObj)\n",
    "\n",
    "# create heatmap layer, weighted by severity\n",
    "heatmap = HeatMap(list(zip(df['Start_Lat'], df['Start_Lng'], df[\"Severity\"])),\n",
    "                   min_opacity=0.5,\n",
    "                   radius=10, blur=10, \n",
    "                   max_zoom=1, name = 'Accident Severity Heatmap')\n",
    "\n",
    "# add heatmap layer to base map\n",
    "heatmap.add_to(mapObj)\n",
    "\n",
    "# add tile for Esri Satellite Imagery\n",
    "tile = folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(mapObj)\n",
    "\n",
    "#add minimap\n",
    "minimap = MiniMap(toggle_display=True, \n",
    "        position=\"bottomleft\", \n",
    "        width=150, \n",
    "        height=100, \n",
    "        tile_layer=\"OpenStreetMap\", \n",
    "        zoom_level_offset=-6)\n",
    "        \n",
    "minimap.add_to(mapObj)\n",
    "\n",
    "#add the option to switch between OpenStreetMap and Esri Satellite Layer\n",
    "folium.LayerControl().add_to(mapObj)\n",
    "\n",
    "mapObj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matplotlib, geopandas and shapely point geometry to create a point map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract state polygon data from census.gov with geopandas.read_file\n",
    "state_df = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_5m.zip\")\n",
    "state_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019.\n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n",
    "\n",
    "\"Motor vehicle accidents are the leading cause of death for people aged 1-54 in the United States.\" - Centers for Disease Control and Prevention. (n.d.). Leading causes of death. Retrieved December 25, 2022, from https://www.cdc.gov/nchs/fastats/leading-causes-of-death.htm\n",
    "\n",
    "\"In 2019, over 36,000 people died in car accidents in the US, and millions more were injured.\" - National Highway Traffic Safety Administration. (2020). Traffic Safety Facts: 2019 Motor Vehicle Crashes: Overview. Retrieved December 25, 2022, from https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812497\n",
    "\n",
    "\"The National Highway Traffic Safety Administration estimates that the economic cost of motor vehicle crashes in the US was over $800 billion in 2020, including costs such as medical expenses, lost productivity, and property damage.\" - National Highway Traffic Safety Administration. (2020). The Economic and Societal Impact of Motor Vehicle Crashes, 2010 (Revised). Retrieved December 25, 2022, from https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/812013-economic_societal_impact_2010.pdf\n",
    "\n",
    "\n",
    "https://smoosavi.org/datasets/us_accidents\n",
    "\n",
    "https://arxiv.org/pdf/1906.05409.pdf \n",
    "\n",
    "https://paperswithcode.com/paper/a-countrywide-traffic-accident-dataset/review/ \n",
    "\n",
    "https://arxiv.org/abs/1909.09638 \n",
    "\n",
    "https://gking.harvard.edu/files/0s.pdf\n",
    "\n",
    "https://pdf.sciencedirectassets.com/308315/1-s2.0-S2352146516X00051/1-s2.0-S235214651630299X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEH8aCXVzLWVhc3QtMSJGMEQCIGvjeArr308aWai7ZIlvy2SouQbVcuEHDlhZ0vipSRAvAiA3dXbOIrBb3bN%2FHCyPirhEi%2FUyg6EopOyhCTZDgc14vyrVBAio%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIM%2F5PyE1uoi44S3jqwKqkENl3HXqL74nPe2Z2PtLnflCl8UbcijNdn7HUyBcY%2BeLog7qMIbrqaiMNjsinlUWSD9nLH17sXgx%2F6GJFmZ37GnBL%2FMvdvQJ4dZAgNSMsL3ppoCi%2F%2BCagUh7tqIvFH7%2BKW0TI1W6cMxjm5%2Bv7ov7JxvVOClRu7U2erwvpZ9FIBvK5ILlbwCDNSxl2m3nNZrA8VIIX6RX1zXJsejOiZIleB6wITF7Ci9LS5M9C1AiDeI%2Fkkt8XW0bVEiWxsFZIoZVM6Ji6DUJinKjKhQJW93lHtDGon2sC5nS%2BcXXwNMdRZI78D97LV4wJw9qabK6lnjAiNKuvkXTV2Lja6h1BaOUKmqmWv3GuW9MBo4cK%2FfZsYtmkqxhhhV14Pa87IkvdJFcQmQZ6Qs7btMuqEUxAl8gB%2FqI0cWhowoL35uOT%2FZ70PJmpWHRpDoiOgRpVOt9%2Ft8LpJV%2FXZcde7bH26vlMgepxy9oodbaV6ccQjQjBp2ye0YZSKdFuwI3cQbY2ptpDHWhrUDDen9g7n4lM0yI%2Fm4R5MIiBPseUoxqITdYwpX%2Bg6lw%2B%2BnDs6PCbUGUS7HWuPbZidM3qpGlXAOUh%2BDylEcaLofsR3BIr4aMQ8jEVSrAxcfX1%2FnTJFzuqLQvb8FbnA4MQoHqrz5kGaNSYoGExzcfxbTR8XzknvehdktEziCc5wpkwtWDRs5gOJVMW7ExIiErkcVsvxgaSVO00lTAJyr5p5oZINHCIk2mIPxzCE89ycBjqqAfvIiqMv4qvi3ZuQkfq2V8rba0HtH0NBld9zevqeNGZtN6X%2FiF6XE4DV5xp%2FR2YonK99XPeqTkPerip2RzB6%2BQ%2FRi%2FXbNzM0wvadjp3inlXXxK6yttR4krt0RbDiFqrMrzpEUXEpjcvBFNvbhOsL6YvXlVEHufeuNEyYtc%2F3EhRQDCm%2B8nn06I%2FJdh2Kt5Q3Sq9miKHi7XD8I52oX%2B8KUvS3lJYVaxfwEUu0&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221212T153730Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUOC55GC7%2F20221212%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=4975f068f7a383d3db89d29b4e55256270c37b6f3cea65612bd414d7d62745e0&hash=b5644175f821ef219444baa69ce7dd1e78143d84d14d753af7fb935fa38509f5&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S235214651630299X&tid=spdf-f033c6e3-936b-4a76-9447-4c4be9474c87&sid=47293284386e994eef08f2a94a40746c1264gxrqa&type=client&ua=55505a5456010f0a02&rr=77878aadb95e82c9\n",
    "\n",
    "https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents \n",
    "\n",
    "https://www.kaggle.com/code/satyabrataroy/60-insights-extraction-us-accident-analysis/notebook\n",
    "\n",
    "https://www2a.cdc.gov/nioshtic-2/BuildQyr.asp?s1=20056884&f1=%2A&Startyear=&Adv=0&terms=1&D1=10&EndYear=&Limit=10000&sort=&PageNo=1&RecNo=1&View=f&\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking \n",
    "\n",
    "https://pygis.io/docs/c_rasters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7a2bde38895aecb6833e1ccd8044599f9803bb162e305adacdb2fce1dea2f67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
